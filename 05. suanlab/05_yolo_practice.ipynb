{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_yolo_practice.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1tSeKrsXh4dsu67vve13iHGJezw5HSoMX","authorship_tag":"ABX9TyMc6Mr4JkxU7PLVukd/MWcY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xL799-9jT1W7"},"source":["- 참조 : https://github.com/wikibook/dl-vision"]},{"cell_type":"markdown","metadata":{"id":"aE7-X_QfSm0G"},"source":["# 객체 탐지 (Object Detection)\n","\n","- 한 이미지에서 객체와 그 경계 상자(bounding box)를 탐지\n","\n","- 객체 탐지 알고리즘은 일반적으로 이미지를 입력으로 받고, 경계 상자와 객체 클래스 리스트를 출력\n","\n","- 경계 상자에 대해 그에 대응하는 예측 클래스와 클래스의 신뢰도(confidence)를 출력\n","\n","## Applications\n","\n","- 자율 주행 자동차에서 다른 자동차와 보행자를 찾을 때\n","\n","- 의료 분야에서 방사선 사진을 사용해 종양이나 위험한 조직을 찾을 때\n","\n","- 제조업에서 조립 로봇이 제품을 조립하거나 수리할 때\n","\n","- 보안 산업에서 위협을 탐지하거나 사람을 셀 때"]},{"cell_type":"markdown","metadata":{"id":"4LGNVEwnfNjn"},"source":["## 용어 설명"]},{"cell_type":"markdown","metadata":{"id":"IZnxALKYfP8s"},"source":["### Boudning Box\n","\n","- 이미지에서 하나의 객체 전체를 포함하는 가장 작은 직사각형\n","\n","  <img src=\"https://miro.medium.com/max/850/1*KL6r494Eyfh3iYEXQA2tzg.png\">\n","\n","  <sub>[이미지 출처] https://medium.com/anolytics/how-bounding-box-annotation-helps-object-detection-in-machine-learning-use-cases-431d93e7b25b</sub>"]},{"cell_type":"markdown","metadata":{"id":"nx5ViAXvwcWS"},"source":["### IOU(Intersection Over Union)\n","- 실측값(Ground Truth)과 모델이 예측한 값이 얼마나 겹치는지를 나타내는 지표\n","\n","  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png\" width=\"300\">\n","\n","- IOU가 높을수록 잘 예측한 모델\n","\n","  <img src=\"https://pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png\" width=\"400\">\n","\n","<br>\n","\n","- 예시\n","  <img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_stop_sign.jpg\">\n","\n","  <sub>[이미지 출처] https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</sub>"]},{"cell_type":"code","metadata":{"id":"bHVxK0LXqd74"},"source":["import numpy as np\n","\n","def compute_iou(pred_box, gt_box):\n","    x1 =np.maximum(pred_box[0], gt_box[0])\n","    y1 =np.maximum(pred_box[1], gt_box[1])\n","    x2 =np.maximum(pred_box[2], gt_box[2])\n","    y2 =np.maximum(pred_box[3], gt_box[3])\n","\n","    intersection = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n","\n","    pred_box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n","    gt_box_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n","\n","    union = pred_box_area + gt_box_area - intersection\n","\n","    iou = intersection / union\n","\n","    return iou"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKL2Ae7SoYDs"},"source":["### NMS(Non-Maximum Suppression, 비최댓값 억제)\n","\n","- 확률이 가장 높은 상자와 겹치는 상자들을 제거하는 과정\n","\n","- 최댓값을 갖지 않는 상자들을 제거\n","\n","- 과정\n","\n","  1. 확률 기준으로 모든 상자를 정렬하고 먼저 가장 확률이 높은 상자를 취함\n","\n","  2. 각 상자에 대해 다른 모든 상자와의 IOU를 계산\n","\n","  3. 특정 임곗값을 넘는 상자는 제거\n","\n","  <img src=\"https://pyimagesearch.com/wp-content/uploads/2014/10/nms_fast_03.jpg\">\n","\n","  <sub>[이미지 출처] https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/</sub>"]},{"cell_type":"code","metadata":{"id":"WTh_WYA0tUBz"},"source":["import numpy as np\n","\n","def non_max_suppresion_fast(boxes, overlap_thresh):\n","    if len(boxes) == 0:\n","        return []\n","    \n","    if boxes.dtype.kind == 'i': # 캐스팅\n","        boxes = boxes.astype('float')\n","    \n","    pick = []\n","    x1 = boxes[:, 0]\n","    y1 = boxes[:, 1]\n","    x2 = boxes[:, 2]\n","    y2 = boxes[:, 3]\n","\n","    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n","    idxs = np.argsort(y2)\n","\n","    while len(idxs) > 0:\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","\n","        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n","        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n","        xx2 = np.maximum(x2[i], x2[idxs[:last]])\n","        yy2 = np.maximum(y2[i], y2[idxs[:last]])\n","\n","        w = np.maximum(0, xx2 - xx1 + 1)\n","        h = np.maximum(0, yy2 - yy1 + 1)\n","\n","        overlap = (w * h) / area(idxs[:last])\n","\n","        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlap_thresh)[0])))\n","    \n","    return boxes[pick].astype('int')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSLe74EnU6zQ"},"source":["## 모델 성능 평가"]},{"cell_type":"markdown","metadata":{"id":"5zMP3ecSU1pa"},"source":["### 정밀도와 재현율\n","\n","- 일반적으로 객체 탐지 모델 평가에 사용되지는 않지만, 다른 지표를 계산하는 기본 지표 역할을 함\n","\n","  - `TP` \n","\n","    - True Positives\n","\n","    - 예측이 동일 클래스의 실제 상자와 일치하는지 측정\n","\n","  - `FP`\n","\n","    - False Positives\n","\n","    - 예측이 실제 상자와 일치하지 않는지 측정\n","\n","  - `FN`\n","\n","    - False Negatives\n","\n","    - 실제 분류값이 그와 일치하는 예측을 갖지 못하는지 측정\n","\n","<br>\n","\n","## $\\qquad precision = \\frac{TP}{TP \\ + \\ FP}$\n","## $\\qquad recall = \\frac{TP}{TP \\ + \\ FN}$\n","\n","  - 모델이 안정적이지 않은 특징을 기반으로 객체 존재를 예측하면 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n","\n","  - 모델이 너무 엄격해서 정확한 조건을 만족할 때만 객체가 탐지된 것으로 간주하면 거짓부정(FN)이 많아져서 재현율이 낮아짐\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kTDOrbKYkSyh"},"source":["### 정밀도-재현율 곡선(precision-recall curve) \n","\n","- 신뢰도 임곗값마다 모델의 정밀도와 재현율을 시각화\n","\n","- 모든 bounding box와 함께 모델이 예측의 정확성을 얼마나 확실하는지 0 ~ 1사이의 숫자로 나타내는 신뢰도를 출력\n","\n","- 임계값 T에 따라 정밀도와 재현율이 달라짐\n","\n","  - 임곗값 T 이하의 예측은 제거함\n","\n","  - T가 1에 가까우면 정밀도는 높지만 재현율은 낮음  \n","    놓치는 객체가 많아져서 재현율이 낮아짐. 즉, 신뢰도가 높은 예측만 유지하기때문에 정밀도는 높아짐\n","\n","  - T가 0에 가까우면 정밀도는 낮지만 재현율은 높음  \n","    대부분의 예측을 유지하기때문에 재현율은 높아지고, 거짓긍정(FP)이 많아져서 정밀도가 낮아짐\n","\n","- 예를 들어, 모델이 보행자를 탐지하고 있으면 특별한 이유없이 차를 세우더라도 어떤 보행자도 놓치지 않도록 재현율을 높여야 함\n","- 모델이 투자 기회를 탐지하고 있다면 일부 기회를 놓치게 되더라도 잘못된 기회에 돈을 거는 일을 피하기 위해 정밀도를 높여야 함\n","\n","<img src=\"https://www.researchgate.net/profile/Davide_Chicco/publication/321672019/figure/fig1/AS:614279602511886@1523467078452/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the.png\">\n","\n","<sub>[이미지 출처] https://www.researchgate.net/figure/a-Example-of-Precision-Recall-curve-with-the-precision-score-on-the-y-axis-and-the_fig1_321672019</sub>"]},{"cell_type":"markdown","metadata":{"id":"9TNopqHBmh2T"},"source":["### AP (Average Precision, 평균 정밀도) 와 mAP(mean Average Precision)\n","\n","- 곡선의 아래 영역에 해당\n","\n","- 항상 1x1 정사각형으로 구성되어 있음  \n","  즉, 항상 0 ~ 1 사이의 값을 가짐\n","\n","- 단일 클래스에 대한 모델 성능 정보를 제공\n","\n","- 전역 점수를 얻기위해서 mAP를 사용\n","\n","- 예를 들어, 데이터셋이 10개의 클래스로 구성된다면 각 클래스에 대한 AP를 계산하고, 그 숫자들의 평균을 다시 구함\n","\n","- (참고)\n","\n","  - 최소 2개 이상의 객체를 탐지하는 대회인 PASCAL Visual Object Classes와 Common Objects in Context(COCO)에서 mAP가 사용됨\n","\n","  - COCO 데이터셋이 더 많은 클래스를 포함하고 있기 때문에 보통 Pascal VOC보다 점수가 더 낮게 나옴\n","\n","  - 예시\n","\n","    <img src=\"https://www.researchgate.net/profile/Bong_Nam_Kang/publication/328939155/figure/tbl2/AS:692891936649218@1542209719916/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev.png\">\n","\n","    <sub>[이미지 출처] https://www.researchgate.net/figure/Evaluation-on-PASCAL-VOC-2007-and-MS-COCO-test-dev_tbl2_328939155</sub>"]},{"cell_type":"markdown","metadata":{"id":"KN1jyyGEkTvx"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"Eprofi5rbUuA"},"source":["### VOC\n","\n","- 2005년부터 2012년까지 진행\n","\n","- Object Detection 기술의 benchmark로 간주\n","\n","- 데이터셋에는 20개의 클래스가 존재\n","\n","      background\n","      aeroplane\n","      bicycle\n","      bird\n","      boat\n","      bottle\n","      bus\n","      car\n","      cat\n","      chair\n","      cow\n","      diningtable\n","      dog\n","      horse\n","      motorbike\n","      person\n","      pottedplant\n","      sheep\n","      sofa\n","      train\n","      tvmonitor\n","\n","- 훈련 및 검증 데이터 : 11,530개\n","\n","- ROI에 대한 27,450개의 Annotation이 존재\n","\n","- 이미지당 2.4개의 객체 존재\n","\n","  <img src=\"https://paperswithcode.github.io/sotabench-eval/img/pascalvoc2012.png\">\n","\n","  <sub>[이미지 출처] https://paperswithcode.github.io/sotabench-eval/pascalvoc/</sub>"]},{"cell_type":"markdown","metadata":{"id":"zL1IEr71bWE_"},"source":["### COCO Dataset\n","\n","- Common Objects in Context\n","\n","- 200,000개의 이미지 \n","\n","- 80개의 카테고리에 500,000개 이상의 객체 Annotation이 존재\n","      person\n","      bicycle\n","      car\n","      motorbike\n","      aeroplane\n","      bus\n","      train\n","      truck\n","      boat\n","      traffic light\n","      fire hydrant\n","      stop sign\n","      parking meter\n","      bench\n","      bird\n","      cat\n","      dog\n","      horse\n","      sheep\n","      cow\n","      elephant\n","      bear\n","      zebra\n","      giraffe\n","      backpack\n","      umbrella\n","      handbag\n","      tie\n","      suitcase\n","      frisbee\n","      skis\n","      snowboard\n","      sports ball\n","      kite\n","      baseball bat\n","      baseball glove\n","      skateboard\n","      surfboard\n","      tennis racket\n","      bottle\n","      wine glass\n","      cup\n","      fork\n","      knife\n","      spoon\n","      bowl\n","      banana\n","      apple\n","      sandwich\n","      orange\n","      broccoli\n","      carrot\n","      hot dog\n","      pizza\n","      donut\n","      cake\n","      chair\n","      sofa\n","      pottedplant\n","      bed\n","      diningtable\n","      toilet\n","      tvmonitor\n","      laptop\n","      mouse\n","      remote\n","      keyboard\n","      cell phone\n","      microwave\n","      oven\n","      toaster\n","      sink\n","      refrigerator\n","      book\n","      clock\n","      vase\n","      scissors\n","      teddy bear\n","      hair drier\n","      toothbrush\n","- https://cocodataset.org/#home\n","\n","<img src=\"https://cocodataset.org/images/coco-examples.jpg\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EMmlbrpjeD6e"},"source":["# YOLO (You Only Look Once)\n","\n","- 가장 빠른 객체 검출 알고리즘 중 하나\n","\n","- 256x256 사이즈의 이미지\n","\n","- GPU 사용 시, 초당 170프레임(170**FPS**, **frames per second**),  \n","  이는 파이썬, 텐서플로 기반 프레임워크가 아닌 C++로 구현된 코드 기준\n","\n","- 작은 크기의 물체를 탐지하는데는 어려움\n","\n","<img src=\"https://miro.medium.com/max/1400/1*bSLNlG7crv-p-m4LVYYk3Q.png\" width=\"600\">\n","\n","\n","- https://pjreddie.com/darknet/yolo/\n","\n","- 자세한 내용 참조 : https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow"]},{"cell_type":"markdown","metadata":{"id":"txORxnfIAegL"},"source":["## YOLO Backbone\n","\n","- 백본 모델(backbone model) 기반\n","\n","- 특징 추출기(Feature Extractor)라고도 불림\n","\n","- YOLO는 자체 맞춤 아키텍쳐 사용\n","\n","- 어떤 특징 추출기 아키텍쳐를 사용했는지에 따라 성능 달라짐\n","\n","  <img src=\"https://www.researchgate.net/publication/335865923/figure/fig1/AS:804106595758082@1568725360777/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three.jpg\">\n","\n","  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-detail-of-YOLOv3It-uses-Darknet-53-as-the-backbone-network-and-uses-three_fig1_335865923</sub>\n","\n","- 마지막 계층은 크기가 $w \\times h \\times D$인 특징 볼륨 출력\n","\n","- $w \\times h $는 그리드의 크기이고, $D$는 특징 볼륨 깊이\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7qWbjby_DwuF"},"source":["## YOLO의 계층 출력\n","\n","- 마지막 계층 출력은 $w \\times h \\times M$ 행렬\n","  \n","  - $M = B \\times (C + 5)$\n","\n","    - B : 그리드 셀당 경계 상자 개수\n","\n","    - C : 클래스 개수\n","\n","  - 클래스 개수에 5를 더한 이유는 해당 값 만큼의 숫자를 예측해야함\n","\n","    - $t_x$, $t_y$는 경계상자의 중심 좌표를 계산\n","\n","    - $t_w$, $t_h$는 경계상자의 너비와 높이를 계산\n","\n","    - $c$는 객체가 경계 상자 안에 있다고 확신하는 신뢰도\n","\n","    - $p1, p2, ..., pC$는 경계상자가 클래스 1, 2, ..., C의 객체를 포함할 확률\n","  \n","  <br>\n","\n","  <img src=\"https://www.researchgate.net/profile/Thi_Le3/publication/337705605/figure/fig3/AS:831927326089217@1575358339500/Structure-of-one-output-cell-in-YOLO.ppm\">\n","\n","  <sub>[이미지 출처] https://www.researchgate.net/figure/Structure-of-one-output-cell-in-YOLO_fig3_337705605</sub>"]},{"cell_type":"markdown","metadata":{"id":"AarR-8M1kkAc"},"source":["## 앵커 박스(Anchor Box)\n","\n","- YOLOv2에서 도입\n","\n","- 사전 정의된 상자(prior box)\n","\n","- 객체에 가장 근접한 앵커 박스를 맞추고 신경망을 사용해 앵커 박스의 크기를 조정하는 과정때문에 $t_x, t_y, t_w, t_h$이 필요\n","\n","  <img src=\"https://kr.mathworks.com/help/vision/ug/ssd_detection.png\">\n","\n","  <sub>[이미지 출처] https://kr.mathworks.com/help/vision/ug/getting-started-with-yolo-v2.html</sub>"]},{"cell_type":"markdown","metadata":{"id":"CdToZZ3xokiM"},"source":["# YOLOv3 Inference 연습 : tensorflow 2\n","\n","- 코드 참조 : https://github.com/zzh8829/yolov3-tf2"]},{"cell_type":"markdown","metadata":{"id":"3B8C3YpS4n2a"},"source":["## Clone and install dependencies"]},{"cell_type":"code","metadata":{"id":"qb3jBxF9x-JR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651466587725,"user_tz":-540,"elapsed":10618,"user":{"displayName":"박재훈","userId":"10151351399794725183"}},"outputId":"009d5b6a-b508-4745-feee-9e2f91d27c35"},"source":["!git clone https://github.com/zzh8829/yolov3-tf2\n","%cd yolov3-tf2/\n","!pip install -r requirements-gpu.txt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov3-tf2'...\n","remote: Enumerating objects: 439, done.\u001b[K\n","remote: Counting objects: 100% (27/27), done.\u001b[K\n","remote: Compressing objects: 100% (20/20), done.\u001b[K\n","remote: Total 439 (delta 11), reused 17 (delta 7), pack-reused 412\u001b[K\n","Receiving objects: 100% (439/439), 4.25 MiB | 15.75 MiB/s, done.\n","Resolving deltas: 100% (248/248), done.\n","/content/yolov3-tf2/yolov3-tf2\n","Obtaining file:///content/yolov3-tf2/yolov3-tf2 (from -r requirements-gpu.txt (line 6))\n","Requirement already satisfied: tensorflow-gpu==2.5.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 1)) (2.5.1)\n","Requirement already satisfied: opencv-python==4.2.0.32 in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 2)) (4.2.0.32)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 3)) (4.2.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements-gpu.txt (line 4)) (4.64.0)\n","Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.12.1)\n","Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2.5.0.dev2021032900)\n","Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.1.2)\n","Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.6.3)\n","Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.2.0)\n","Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.12)\n","Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.15.0)\n","Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.1.0)\n","Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.19.5)\n","Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.15.0)\n","Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2.5.0)\n","Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.37.1)\n","Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.3.0)\n","Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.1.0)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.17.3)\n","Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.34.1)\n","Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.4.0)\n","Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2.8.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.5.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (57.4.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.0.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2.23.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.35.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.3.6)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.4.6)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (4.11.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow-gpu==2.5.1->-r requirements-gpu.txt (line 1)) (3.2.0)\n","Installing collected packages: yolov3-tf2\n","  Attempting uninstall: yolov3-tf2\n","    Found existing installation: yolov3-tf2 0.1\n","    Can't uninstall 'yolov3-tf2'. No files were found to uninstall.\n","  Running setup.py develop for yolov3-tf2\n","Successfully installed yolov3-tf2-0.1\n"]}]},{"cell_type":"code","metadata":{"id":"FONrNLUIx-ML","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651465813271,"user_tz":-540,"elapsed":363,"user":{"displayName":"박재훈","userId":"10151351399794725183"}},"outputId":"314590e5-489f-465a-ce25-bed1383fda53"},"source":["!ls"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["checkpoints\t data\t\t  README.md\t\ttrain.py\n","colab_gpu.ipynb  detect.py\t  requirements-gpu.txt\tyolov3_tf2\n","conda-cpu.yml\t detect_video.py  requirements.txt\tyolov3_tf2.egg-info\n","conda-gpu.yml\t docs\t\t  setup.py\n","convert.py\t LICENSE\t  tools\n"]}]},{"cell_type":"markdown","metadata":{"id":"y2QJVUyc4io-"},"source":["## Check Tensorflow2 version"]},{"cell_type":"code","metadata":{"id":"41me-hpBx-fD","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1651466623209,"user_tz":-540,"elapsed":375,"user":{"displayName":"박재훈","userId":"10151351399794725183"}},"outputId":"3eb3a07e-ca5d-4229-e1b9-63d4bc00d918"},"source":["import tensorflow as tf\n","tf.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.5.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"1Z-L1V51ziJE"},"source":["## Convert Pretrained Darknet Weight\n","\n","- https://pjreddie.com/media/files/yolov3.weights\n","- `yolov3.weights`를 `/yolov3-tf2/data/`로 넣기"]},{"cell_type":"code","metadata":{"id":"hq6yz4Mpx--B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651466646539,"user_tz":-540,"elapsed":12221,"user":{"displayName":"박재훈","userId":"10151351399794725183"}},"outputId":"feb534ad-b980-41c3-9fa3-19d1a8f4f87c"},"source":["!wget https://pjreddie.com/media/files/yolov3.weights -O data/yolov3.weights\n","!python convert.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-02 04:43:53--  https://pjreddie.com/media/files/yolov3.weights\n","Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n","Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 248007048 (237M) [application/octet-stream]\n","Saving to: ‘data/yolov3.weights’\n","\n","data/yolov3.weights 100%[===================>] 236.52M  57.4MB/s    in 4.4s    \n","\n","2022-05-02 04:43:58 (54.3 MB/s) - ‘data/yolov3.weights’ saved [248007048/248007048]\n","\n","2022-05-02 04:43:58.697534: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2022-05-02 04:44:01.122038: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n","2022-05-02 04:44:01.130960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.131856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n","coreClock: 0.562GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n","2022-05-02 04:44:01.131918: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2022-05-02 04:44:01.136195: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n","2022-05-02 04:44:01.136280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n","2022-05-02 04:44:01.139349: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n","2022-05-02 04:44:01.140016: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n","2022-05-02 04:44:01.145090: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n","2022-05-02 04:44:01.146131: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n","2022-05-02 04:44:01.146376: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n","2022-05-02 04:44:01.146520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.147284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.148096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n","2022-05-02 04:44:01.208521: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-05-02 04:44:01.208922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.209756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n","pciBusID: 0000:00:04.0 name: Tesla K80 computeCapability: 3.7\n","coreClock: 0.562GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s\n","2022-05-02 04:44:01.209858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.210554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.211224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n","2022-05-02 04:44:01.211294: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n","2022-05-02 04:44:01.777228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n","2022-05-02 04:44:01.777297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n","2022-05-02 04:44:01.777322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n","2022-05-02 04:44:01.777539: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.778333: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.779056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-05-02 04:44:01.779789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9639 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","W0502 04:44:03.935163 139906017441664 deprecation.py:534] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n","Instructions for updating:\n","The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n","Model: \"yolov3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input (InputLayer)              [(None, None, None,  0                                            \n","__________________________________________________________________________________________________\n","yolo_darknet (Functional)       ((None, None, None,  40620640    input[0][0]                      \n","__________________________________________________________________________________________________\n","yolo_conv_0 (Functional)        (None, None, None, 5 11024384    yolo_darknet[0][2]               \n","__________________________________________________________________________________________________\n","yolo_conv_1 (Functional)        (None, None, None, 2 2957312     yolo_conv_0[0][0]                \n","                                                                 yolo_darknet[0][1]               \n","__________________________________________________________________________________________________\n","yolo_conv_2 (Functional)        (None, None, None, 1 741376      yolo_conv_1[0][0]                \n","                                                                 yolo_darknet[0][0]               \n","__________________________________________________________________________________________________\n","yolo_output_0 (Functional)      (None, None, None, 3 4984063     yolo_conv_0[0][0]                \n","__________________________________________________________________________________________________\n","yolo_output_1 (Functional)      (None, None, None, 3 1312511     yolo_conv_1[0][0]                \n","__________________________________________________________________________________________________\n","yolo_output_2 (Functional)      (None, None, None, 3 361471      yolo_conv_2[0][0]                \n","__________________________________________________________________________________________________\n","yolo_boxes_0 (Lambda)           ((None, None, None,  0           yolo_output_0[0][0]              \n","__________________________________________________________________________________________________\n","yolo_boxes_1 (Lambda)           ((None, None, None,  0           yolo_output_1[0][0]              \n","__________________________________________________________________________________________________\n","yolo_boxes_2 (Lambda)           ((None, None, None,  0           yolo_output_2[0][0]              \n","__________________________________________________________________________________________________\n","yolo_nms (Lambda)               ((1, None, 4), (1, N 0           yolo_boxes_0[0][0]               \n","                                                                 yolo_boxes_0[0][1]               \n","                                                                 yolo_boxes_0[0][2]               \n","                                                                 yolo_boxes_1[0][0]               \n","                                                                 yolo_boxes_1[0][1]               \n","                                                                 yolo_boxes_1[0][2]               \n","                                                                 yolo_boxes_2[0][0]               \n","                                                                 yolo_boxes_2[0][1]               \n","                                                                 yolo_boxes_2[0][2]               \n","==================================================================================================\n","Total params: 62,001,757\n","Trainable params: 61,949,149\n","Non-trainable params: 52,608\n","__________________________________________________________________________________________________\n","I0502 04:44:03.959282 139906017441664 convert.py:24] model created\n","I0502 04:44:03.960446 139906017441664 utils.py:45] yolo_darknet/conv2d bn\n","I0502 04:44:03.963363 139906017441664 utils.py:45] yolo_darknet/conv2d_1 bn\n","I0502 04:44:03.966156 139906017441664 utils.py:45] yolo_darknet/conv2d_2 bn\n","I0502 04:44:03.968828 139906017441664 utils.py:45] yolo_darknet/conv2d_3 bn\n","I0502 04:44:03.971570 139906017441664 utils.py:45] yolo_darknet/conv2d_4 bn\n","I0502 04:44:03.974837 139906017441664 utils.py:45] yolo_darknet/conv2d_5 bn\n","I0502 04:44:03.977539 139906017441664 utils.py:45] yolo_darknet/conv2d_6 bn\n","I0502 04:44:03.980559 139906017441664 utils.py:45] yolo_darknet/conv2d_7 bn\n","I0502 04:44:03.983236 139906017441664 utils.py:45] yolo_darknet/conv2d_8 bn\n","I0502 04:44:03.986226 139906017441664 utils.py:45] yolo_darknet/conv2d_9 bn\n","I0502 04:44:03.990973 139906017441664 utils.py:45] yolo_darknet/conv2d_10 bn\n","I0502 04:44:03.993871 139906017441664 utils.py:45] yolo_darknet/conv2d_11 bn\n","I0502 04:44:03.997631 139906017441664 utils.py:45] yolo_darknet/conv2d_12 bn\n","I0502 04:44:04.000431 139906017441664 utils.py:45] yolo_darknet/conv2d_13 bn\n","I0502 04:44:04.004204 139906017441664 utils.py:45] yolo_darknet/conv2d_14 bn\n","I0502 04:44:04.006862 139906017441664 utils.py:45] yolo_darknet/conv2d_15 bn\n","I0502 04:44:04.010582 139906017441664 utils.py:45] yolo_darknet/conv2d_16 bn\n","I0502 04:44:04.013108 139906017441664 utils.py:45] yolo_darknet/conv2d_17 bn\n","I0502 04:44:04.017093 139906017441664 utils.py:45] yolo_darknet/conv2d_18 bn\n","I0502 04:44:04.019846 139906017441664 utils.py:45] yolo_darknet/conv2d_19 bn\n","I0502 04:44:04.023927 139906017441664 utils.py:45] yolo_darknet/conv2d_20 bn\n","I0502 04:44:04.026758 139906017441664 utils.py:45] yolo_darknet/conv2d_21 bn\n","I0502 04:44:04.031013 139906017441664 utils.py:45] yolo_darknet/conv2d_22 bn\n","I0502 04:44:04.033691 139906017441664 utils.py:45] yolo_darknet/conv2d_23 bn\n","I0502 04:44:04.037535 139906017441664 utils.py:45] yolo_darknet/conv2d_24 bn\n","I0502 04:44:04.040502 139906017441664 utils.py:45] yolo_darknet/conv2d_25 bn\n","I0502 04:44:04.045048 139906017441664 utils.py:45] yolo_darknet/conv2d_26 bn\n","I0502 04:44:04.055502 139906017441664 utils.py:45] yolo_darknet/conv2d_27 bn\n","I0502 04:44:04.058965 139906017441664 utils.py:45] yolo_darknet/conv2d_28 bn\n","I0502 04:44:04.067301 139906017441664 utils.py:45] yolo_darknet/conv2d_29 bn\n","I0502 04:44:04.070719 139906017441664 utils.py:45] yolo_darknet/conv2d_30 bn\n","I0502 04:44:04.078981 139906017441664 utils.py:45] yolo_darknet/conv2d_31 bn\n","I0502 04:44:04.082617 139906017441664 utils.py:45] yolo_darknet/conv2d_32 bn\n","I0502 04:44:04.090715 139906017441664 utils.py:45] yolo_darknet/conv2d_33 bn\n","I0502 04:44:04.094516 139906017441664 utils.py:45] yolo_darknet/conv2d_34 bn\n","I0502 04:44:04.102910 139906017441664 utils.py:45] yolo_darknet/conv2d_35 bn\n","I0502 04:44:04.106698 139906017441664 utils.py:45] yolo_darknet/conv2d_36 bn\n","I0502 04:44:04.114809 139906017441664 utils.py:45] yolo_darknet/conv2d_37 bn\n","I0502 04:44:04.118314 139906017441664 utils.py:45] yolo_darknet/conv2d_38 bn\n","I0502 04:44:04.131344 139906017441664 utils.py:45] yolo_darknet/conv2d_39 bn\n","I0502 04:44:04.134878 139906017441664 utils.py:45] yolo_darknet/conv2d_40 bn\n","I0502 04:44:04.152586 139906017441664 utils.py:45] yolo_darknet/conv2d_41 bn\n","I0502 04:44:04.156961 139906017441664 utils.py:45] yolo_darknet/conv2d_42 bn\n","I0502 04:44:04.165976 139906017441664 utils.py:45] yolo_darknet/conv2d_43 bn\n","I0502 04:44:04.198127 139906017441664 utils.py:45] yolo_darknet/conv2d_44 bn\n","I0502 04:44:04.204079 139906017441664 utils.py:45] yolo_darknet/conv2d_45 bn\n","I0502 04:44:04.231279 139906017441664 utils.py:45] yolo_darknet/conv2d_46 bn\n","I0502 04:44:04.236855 139906017441664 utils.py:45] yolo_darknet/conv2d_47 bn\n","I0502 04:44:04.262537 139906017441664 utils.py:45] yolo_darknet/conv2d_48 bn\n","I0502 04:44:04.267860 139906017441664 utils.py:45] yolo_darknet/conv2d_49 bn\n","I0502 04:44:04.292816 139906017441664 utils.py:45] yolo_darknet/conv2d_50 bn\n","I0502 04:44:04.298187 139906017441664 utils.py:45] yolo_darknet/conv2d_51 bn\n","I0502 04:44:04.322566 139906017441664 utils.py:45] yolo_conv_0/conv2d_52 bn\n","I0502 04:44:04.327420 139906017441664 utils.py:45] yolo_conv_0/conv2d_53 bn\n","I0502 04:44:04.352857 139906017441664 utils.py:45] yolo_conv_0/conv2d_54 bn\n","I0502 04:44:04.357895 139906017441664 utils.py:45] yolo_conv_0/conv2d_55 bn\n","I0502 04:44:04.382312 139906017441664 utils.py:45] yolo_conv_0/conv2d_56 bn\n","I0502 04:44:04.387442 139906017441664 utils.py:45] yolo_output_0/conv2d_57 bn\n","I0502 04:44:04.411237 139906017441664 utils.py:45] yolo_output_0/conv2d_58 bias\n","I0502 04:44:04.414224 139906017441664 utils.py:45] yolo_conv_1/conv2d_59 bn\n","I0502 04:44:04.417054 139906017441664 utils.py:45] yolo_conv_1/conv2d_60 bn\n","I0502 04:44:04.419966 139906017441664 utils.py:45] yolo_conv_1/conv2d_61 bn\n","I0502 04:44:04.427734 139906017441664 utils.py:45] yolo_conv_1/conv2d_62 bn\n","I0502 04:44:04.430689 139906017441664 utils.py:45] yolo_conv_1/conv2d_63 bn\n","I0502 04:44:04.438029 139906017441664 utils.py:45] yolo_conv_1/conv2d_64 bn\n","I0502 04:44:04.440918 139906017441664 utils.py:45] yolo_output_1/conv2d_65 bn\n","I0502 04:44:04.448816 139906017441664 utils.py:45] yolo_output_1/conv2d_66 bias\n","I0502 04:44:04.454448 139906017441664 utils.py:45] yolo_conv_2/conv2d_67 bn\n","I0502 04:44:04.456912 139906017441664 utils.py:45] yolo_conv_2/conv2d_68 bn\n","I0502 04:44:04.459443 139906017441664 utils.py:45] yolo_conv_2/conv2d_69 bn\n","I0502 04:44:04.462682 139906017441664 utils.py:45] yolo_conv_2/conv2d_70 bn\n","I0502 04:44:04.464868 139906017441664 utils.py:45] yolo_conv_2/conv2d_71 bn\n","I0502 04:44:04.468250 139906017441664 utils.py:45] yolo_conv_2/conv2d_72 bn\n","I0502 04:44:04.470548 139906017441664 utils.py:45] yolo_output_2/conv2d_73 bn\n","I0502 04:44:04.473955 139906017441664 utils.py:45] yolo_output_2/conv2d_74 bias\n","I0502 04:44:04.475198 139906017441664 convert.py:27] weights loaded\n","2022-05-02 04:44:04.480979: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n","2022-05-02 04:44:04.696872: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n","2022-05-02 04:44:04.699588: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n","Traceback (most recent call last):\n","  File \"convert.py\", line 39, in <module>\n","    app.run(main)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 312, in run\n","    _run_main(main, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 258, in _run_main\n","    sys.exit(main(argv))\n","  File \"convert.py\", line 30, in main\n","    output = yolo(img)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\n","    outputs = call_fn(inputs, *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\", line 421, in call\n","    inputs, training=training, mask=mask)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\", line 556, in _run_internal_graph\n","    outputs = node.layer(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\n","    outputs = call_fn(inputs, *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\", line 421, in call\n","    inputs, training=training, mask=mask)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\", line 556, in _run_internal_graph\n","    outputs = node.layer(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\", line 1030, in __call__\n","    outputs = call_fn(inputs, *args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/convolutional.py\", line 249, in call\n","    outputs = self._convolution_op(inputs, self.kernel)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\", line 206, in wrapper\n","    return target(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1019, in convolution_v2\n","    name=name)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 1149, in convolution_internal\n","    name=name)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 2603, in _conv2d_expanded_batch\n","    name=name)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 932, in conv2d\n","    _ops.raise_from_not_ok_status(e, name)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\", line 6897, in raise_from_not_ok_status\n","    six.raise_from(core._status_to_exception(e.code, message), None)\n","  File \"<string>\", line 3, in raise_from\n","tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [Op:Conv2D]\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mcz5iiyrzY9j"},"source":["## Initial Detector"]},{"cell_type":"code","metadata":{"id":"8Rt1bEf4x-7e"},"source":["import sys\n","from absl import app, logging, flags\n","from absl.flags import FLAGS\n","import time\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from yolov3_tf2.models import YoloV3, YoloV3Tiny\n","from yolov3_tf2.dataset import transform_images, load_tfrecord_dataset\n","from yolov3_tf2.utils import draw_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1f5SWy4x-5I"},"source":["flags.DEFINE_string('classes', './data/coco.names', 'path to classes file')\n","flags.DEFINE_string('weights', './checkpoints/yolov3.tf', \n","                    'path to weights file')\n","flags.DEFINE_boolean('tiny', False, 'yolov3 or yolov3-tiny')\n","flags.DEFINE_integer('size', 416, 'resize images to')\n","flags.DEFINE_string('image', './data/girl.png', 'path to input image')\n","flags.DEFINE_string('tfrecord', None, 'tfrecord instead of image')\n","flags.DEFINE_string('output', './output.jpg', 'path to output image')\n","flags.DEFINE_integer('num_classes', 80, 'number of classes in the model')\n","\n","app._run_init(['yolov3'], app.parse_flags_with_usage)\n","\n","physical_devices = tf.config.experimental.list_physical_devices('GPU')\n","tf.config.experimental.set_memory_growth(physical_devices[0], True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qBaqsEG41l_"},"source":["## Detect Image"]},{"cell_type":"code","metadata":{"id":"LbPOCGuxx-2t"},"source":["FLAGS.image = 'data/meme.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hsa4Dklsx-0_","colab":{"base_uri":"https://localhost:8080/","height":453},"executionInfo":{"status":"error","timestamp":1651466301665,"user_tz":-540,"elapsed":2791,"user":{"displayName":"박재훈","userId":"10151351399794725183"}},"outputId":"fea67d77-0b24-4c7d-8b5f-b770f5513fa2"},"source":["if FLAGS.tiny:\n","    yolo = YoloV3Tiny(classes=FLAGS.num_classes)\n","else:\n","    yolo = YoloV3(classes=FLAGS.num_classes)\n","      \n","yolo.load_weights(FLAGS.weights).expect_partial()\n","logging.info('weights loaded')\n","\n","class_names = [c.strip() for c in open(FLAGS.classes).readlines()]\n","logging.info('classes loaded')"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/yolov3.tf","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-1973340ba6b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0myolo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYoloV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpect_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights loaded'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2288\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2914\u001b[0m   \u001b[0;31m# directory. It's possible for filepath to be both a prefix and directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m   \u001b[0;31m# Prioritize checkpoint over SavedModel.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2916\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2917\u001b[0m     \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2918\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msm_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_readable_tf_checkpoint\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2935\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_readable_tf_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m     \u001b[0mpy_checkpoint_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0merror_translator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36merror_translator\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0;34m'Failed to find any '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       'matching files for') in error_message:\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   elif 'Sliced checkpoints are not supported' in error_message or (\n\u001b[1;32m     37\u001b[0m       \u001b[0;34m'Data type '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to find any matching files for ./checkpoints/yolov3.tf"]}]},{"cell_type":"code","metadata":{"id":"pkvDwZsux-yN"},"source":["img_raw = tf.image.decode_image(open(FLAGS.image, 'rb').read(), channels=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfKcTD_Ax-v7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oLr782wcsFVT"},"source":["# YOLOv3 inference 연습 : Pytorch \n","\n","- video inference\n","- 코드 참조 : https://towardsdatascience.com/yolov3-pytorch-on-google-colab-c4a79eeecdea\n","- https://github.com/ultralytics/yolov3"]},{"cell_type":"markdown","metadata":{"id":"yK71PKqk4aTH"},"source":["## Prepare YOLOv3-pytorch"]},{"cell_type":"code","metadata":{"id":"HrC5-LmG3F6E"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tgl_VY_Tq97I"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R897GokLq95E"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NfJM0tw3TFa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mrlt91dYq915"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0lpUVOdq9zn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbUVelkiszUD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wwhHYatUq9xx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PlEcyGxh3iOI"},"source":["## Git clone to get short videos\n","\n","- https://github.com/vindruid/yolov3-in-colab.git"]},{"cell_type":"code","metadata":{"id":"WoslGlSkq9vo"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pRqM_huM3pIS"},"source":["## Process Video"]},{"cell_type":"code","metadata":{"id":"WNVhPZ0Jq9s5"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LC2axiin3nja"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUldkiZA3wQt"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EflrkImk3rqF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gd4cy_Gd30gu"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"39Nk92gJ5a4J"},"source":["- Custom Data Train 참고\n","\n","  - https://github.com/AntonMu/TrainYourOwnYOLO"]}]}