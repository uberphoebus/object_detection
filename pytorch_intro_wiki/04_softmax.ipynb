{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀의 비용함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2c6553ed690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로우-레벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0900, 0.2447, 0.6652]), tensor(1.))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])\n",
    "h = F.softmax(z, dim=0)\n",
    "h, h.sum() # 소프트맥스 함수의 출력합은 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
       "         [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
       "         [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "h = F.softmax(z, dim=1)\n",
    "h, h.sum(axis=1) # 행별 원소합 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 2, 1]),\n",
       " tensor([[0],\n",
       "         [2],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randint(5, (3, )).long()\n",
    "y, y.unsqueeze(1) # (3,) -> (3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(h) # 3x5 텐서생성\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1) # dim, 위치, 넣을 숫자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(h)).sum(dim=1).mean()\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이-레벨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. F.softmax() + torch.log() = F.log_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. F.log_softmax() + F.nll_loss = F.cross_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * -torch.log(h)).sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(F.log_softmax(z, dim=1), y) # 원핫벡터 없이 실제값 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4]), torch.Size([8]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 로우-레벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "y_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/10, cost: 1.098612\n",
      "epoch 1/10, cost: 1.032776\n",
      "epoch 2/10, cost: 0.985576\n",
      "epoch 3/10, cost: 0.967064\n",
      "epoch 4/10, cost: 0.966850\n",
      "epoch 5/10, cost: 0.985291\n",
      "epoch 6/10, cost: 1.032398\n",
      "epoch 7/10, cost: 1.047309\n",
      "epoch 8/10, cost: 1.091330\n",
      "epoch 9/10, cost: 1.030259\n",
      "epoch 10/10, cost: 1.069288\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "o = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs + 1):\n",
    "    h = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    c = (y_one_hot * -torch.log(h)).sum(dim=1).mean()\n",
    "    \n",
    "    o.zero_grad()\n",
    "    c.backward()\n",
    "    o.step()\n",
    "    \n",
    "    print(f'epoch {epoch}/{epochs}, cost: {c.item():6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이-레벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/10, cost: 1.098612\n",
      "epoch 1/10, cost: 1.032775\n",
      "epoch 2/10, cost: 0.985576\n",
      "epoch 3/10, cost: 0.967064\n",
      "epoch 4/10, cost: 0.966850\n",
      "epoch 5/10, cost: 0.985291\n",
      "epoch 6/10, cost: 1.032399\n",
      "epoch 7/10, cost: 1.047309\n",
      "epoch 8/10, cost: 1.091330\n",
      "epoch 9/10, cost: 1.030258\n",
      "epoch 10/10, cost: 1.069288\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "o = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs + 1):\n",
    "    z = x_train.matmul(W) + b\n",
    "    c = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    o.zero_grad()\n",
    "    c.backward()\n",
    "    o.step()\n",
    "    \n",
    "    print(f'epoch {epoch}/{epochs}, cost: {c.item():6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/10, cost: 1.849513\n",
      "epoch 1/10, cost: 1.298513\n",
      "epoch 2/10, cost: 1.046181\n",
      "epoch 3/10, cost: 0.954982\n",
      "epoch 4/10, cost: 0.929780\n",
      "epoch 5/10, cost: 0.908864\n",
      "epoch 6/10, cost: 0.891565\n",
      "epoch 7/10, cost: 0.877385\n",
      "epoch 8/10, cost: 0.866827\n",
      "epoch 9/10, cost: 0.859866\n",
      "epoch 10/10, cost: 0.859701\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(4, 3) # 입력개수, 클래스개수\n",
    "o = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs + 1):\n",
    "    pred = model(x_train)\n",
    "    c = F.cross_entropy(pred, y_train)\n",
    "    \n",
    "    o.zero_grad()\n",
    "    c.backward()\n",
    "    o.step()\n",
    "    \n",
    "    print(f'epoch {epoch}/{epochs}, cost: {c.item():6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/10, cost: 1.845720\n",
      "epoch 1/10, cost: 1.460510\n",
      "epoch 2/10, cost: 1.265032\n",
      "epoch 3/10, cost: 1.231668\n",
      "epoch 4/10, cost: 1.214960\n",
      "epoch 5/10, cost: 1.176336\n",
      "epoch 6/10, cost: 1.106072\n",
      "epoch 7/10, cost: 1.103787\n",
      "epoch 8/10, cost: 1.042308\n",
      "epoch 9/10, cost: 1.056017\n",
      "epoch 10/10, cost: 0.989940\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "o = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs + 1):\n",
    "    pred = model(x_train)\n",
    "    c = F.cross_entropy(pred, y_train)\n",
    "    \n",
    "    o.zero_grad()\n",
    "    c.backward()\n",
    "    o.step()\n",
    "    \n",
    "    print(f'epoch {epoch}/{epochs}, cost: {c.item():6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀로 MNIST 데이터 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU 연산\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤시드 고정\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(\n",
    "    root='MNIST_data', # 다운로드 경로\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(), # 텐서로 변환\n",
    "    download=True # 데이터셋이 없다면 다운로드\n",
    ")\n",
    "\n",
    "mnist_test = dsets.MNIST(\n",
    "    root='MNIST_data',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=mnist_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True # 미니배치보다 개수가 적은 마지막 배치의 과대평가 방지\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=10, bias=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(784, 10, bias=True).to(device) # to()로 연산메모리 설정\n",
    "linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0/15, cost: 0.535949469\n",
      "epoch 1/15, cost: 0.359360933\n",
      "epoch 2/15, cost: 0.331327111\n",
      "epoch 3/15, cost: 0.316370010\n",
      "epoch 4/15, cost: 0.306923091\n",
      "epoch 5/15, cost: 0.300204843\n",
      "epoch 6/15, cost: 0.294921339\n",
      "epoch 7/15, cost: 0.290828079\n",
      "epoch 8/15, cost: 0.287171513\n",
      "epoch 9/15, cost: 0.284261644\n",
      "epoch 10/15, cost: 0.281829149\n",
      "epoch 11/15, cost: 0.279674202\n",
      "epoch 12/15, cost: 0.277828932\n",
      "epoch 13/15, cost: 0.276153117\n",
      "epoch 14/15, cost: 0.274277598\n",
      "learning finished\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(dataloader)\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        X = X.view(-1, 28 * 28).to(device) # (100, 784)\n",
    "        y = y.to(device) # 클래스(원핫x)\n",
    "        \n",
    "        hypothesis = linear(X)\n",
    "        optimizer.zero_grad()\n",
    "        cost = criterion(hypothesis, y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_cost += cost / total_batch\n",
    "    print(f'epoch {epoch}/{training_epochs}, cost: {avg_cost:.9f}')\n",
    "\n",
    "print('learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8840000033378601\n",
      "label: 2\n",
      "pred: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\mlProject\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "c:\\workspace\\mlProject\\venv\\lib\\site-packages\\torchvision\\datasets\\mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test data로 모델 테스트\n",
    "with torch.no_grad(): # gradient 계산 x\n",
    "    X_test = mnist_test.data.view(-1, 28 * 28).float().to(device)\n",
    "    y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    pred = linear(X_test)\n",
    "    correct_pred = torch.argmax(pred, 1) == y_test\n",
    "    acc = correct_pred.float().mean()\n",
    "    print(f'accuracy: {acc.item()}')\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "    \n",
    "    print(f'label: {y_single_data.item()}')\n",
    "    single_pred = linear(X_single_data)\n",
    "    print(f'pred: {torch.argmax(single_pred, 1).item()}')\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76888c4a1af33f53bb52de03cc72a37cdcfc80966b4bb342e552f029c3bcc4e2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
